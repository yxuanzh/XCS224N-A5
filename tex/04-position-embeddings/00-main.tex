% \section{Attention exploration}
% \graphicspath{ {images/} }

\section{Position Embeddings Exploration}
% \titledquestion{Position Embeddings Exploration}[6]
\label{sec:pos_enc}

Position embeddings are an important component of the Transformer architecture, allowing the model to differentiate between tokens based on their position in the sequence.
In this question, we'll explore the need for positional embeddings in Transformers and how they can be designed.

Recall that the crucial components of the Transformer architecture are the self-attention layer and the feed-forward neural network layer. 
Given an input tensor $\mathbf{X} \in \mathbb{R}^{T \times d}$, where $T$ is the sequence length and $d$ is the hidden dimension, the self-attention layer computes the following:
\begin{align*}
    \mathbf{Q} &= \mathbf{X}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V \\
    \mathbf{H} &= \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V}
\end{align*}
where $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d}$ are weight matrices, and $\mathbf{H} \in \mathbb{R}^{T \times d}$ is the output.

Next, the feed-forward layer applies the following transformation:
\begin{align*}
    \mathbf{Z} = \text{ReLU}(\mathbf{H}\mathbf{W}_1 + \mathbf{1}\cdot\mathbf{b}_1)\mathbf{W}_2 + \mathbf{1}\cdot\mathbf{b}_2
\end{align*}
where $\mathbf{W}_1, \mathbf{W}_2 \in \mathbb{R}^{d \times d}$ and $\mathbf{b}_1, \mathbf{b}_2 \in \mathbb{R}^{1\times d}$ are weights and biases; $\mathbf{1} \in \mathbb{R}^{T\times 1}$ is a vector of ones\footnote{Outer product with $\mathbf{1}$ represents broadcasting operation and makes feed forward network notations mathematically sound.}; and $\mathbf{Z} \in \mathbb{R}^{T \times d}$ is the final output.

(Note that we have omitted some details of the Transformer architecture for simplicity.)

\begin{enumerate}[(a)]
% \begin{parts}
    % \part[4] \textbf{Permuting the input.}

\item \points{4a} \textbf{Permuting the input.} \label{q_permute_input}
    
    % \begin{subparts}
    \begin{enumerate}[label=\roman*.]
        % \subpart[3]
        \item (2 point) Suppose we permute the input sequence $\mathbf{X}$ such that the tokens are shuffled randomly. This can be represented as multiplication by a permutation matrix $\mathbf{P} \in \mathbb{R}^{T \times T}$, i.e. $\mathbf{X}_{\text{perm}} = \mathbf{P}\mathbf{X}$. (See \href{https://en.wikipedia.org/wiki/Permutation_matrix}{Wikipedia} for a recap on permutation matrices.)

        \textbf{Show} that the output $\mathbf{Z}_{\text{perm}}$ for the permuted input $\mathbf{X}_{\text{perm}}$ will be $\mathbf{Z}_{\text{perm}} = \mathbf{P}\mathbf{Z}$.

        You are given that for any permutation matrix $\mathbf{P}$ and any matrix $\mathbf{A}$, the following hold:
        $\text{softmax}(\mathbf{P}\mathbf{A}\mathbf{P}^{\top}) = \mathbf{P}\ \text{softmax}(\mathbf{A})\ \mathbf{P}^{\top} \quad \text{and}\quad \text{ReLU}(\mathbf{P}\mathbf{A}) = \mathbf{P}\ \text{ReLU}(\mathbf{A})$.


        % \subpart[1] 
        \item (1 point) Think about the implications of the result you derived in part i. \textbf{Explain} why this property of the Transformer model could be problematic when processing text.
        
    \end{enumerate}
    % \end{subparts}

    % \part[2]
    \item \points{4b} \textbf{Position embeddings} are vectors that encode the position of each token in the sequence. They are added to the input word embeddings before feeding them into the Transformer.

    One approach is to generate position embedding using a fixed function of the position and the dimension of the embedding. If the input word embeddings are $\mathbf{X} \in \mathbb{R}^{T \times d}$, the position embeddings $\Phi \in \mathbb{R}^{T \times d}$ are generated as follows:
    \begin{align*}
        \Phi_{(t, 2i)} &= \sin\left({t}/{10000^{2i/d}}\right) \\
        \Phi_{(t, 2i+1)} &= \cos\left({t}/{10000^{2i/d}}\right)
    \end{align*}
    where $t \in \{0, 1, \ldots T-1\}$ and $i \in \{0, 1, \ldots d/2-1\}$\footnote{Here $d$ is assumed even which is typically the case for most models.}.

    Specifically, the position embeddings are added to the input word embeddings:
    \begin{align*}
        \mathbf{X}_{\text{pos}} = \mathbf{X} + \Phi
    \end{align*}

    % \begin{subparts}
    \begin{enumerate}[label=\roman*.]
        % \subpart[1]
        \item (1 point) Do you think the position embeddings will help the issue you identified in part (a)? If yes, explain how and if not, explain why not.

        % \subpart[1]
        \item (1 point) Can the position embeddings for two different tokens in the input sequence be the same? If yes, provide an example. If not, explain why not.

    \end{enumerate}
    % \end{subparts}
% \end{parts}% \titledquestion{Attention exploration}[21]

\end{enumerate}