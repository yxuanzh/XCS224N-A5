\graphicspath{ {images/} }

% real numbers R symbol
% \newcommand{\Real}{\mathbb{R}}
% \newcommand{\Int}{\mathbb{Z}}

% encoder hidden
% \newcommand{\henc}{\bh^{\text{enc}}}
% \newcommand{\hencfw}[1]{\overrightarrow{\henc_{#1}}}
% \newcommand{\hencbw}[1]{\overleftarrow{\henc_{#1}}}

% encoder cell
% \newcommand{\cenc}{\bc^{\text{enc}}}
% \newcommand{\cencfw}[1]{\overrightarrow{\cenc_{#1}}}
% \newcommand{\cencbw}[1]{\overleftarrow{\cenc_{#1}}}

% decoder hidden
% \newcommand{\hdec}{\bh^{\text{dec}}}

% decoder cell
% \newcommand{\cdec}{\bc^{\text{dec}}}

\newcommand{\mingpt}{\texttt{minGPT}\xspace}
\newcommand{\ourmodel}{\texttt{minLinT5}\xspace}

% make it possible to copy/paste from code snippets without strange extra spaces or line numbers:
% https://tex.stackexchange.com/questions/4911/phantom-spaces-in-listings
\lstset{basicstyle=\ttfamily,columns=flexible,numbers=none}

\section{Pretrained Transformer models and knowledge access}
\label{sec:char_enc}
% \begin{parts}

%This part of the assignment serves two purposes.
%The first purpose is to be an introduction to relatively realistic research in NLP: you'll start with a codebase that is functional for one use, and modify it for another.

%The second purpose is to introduce you to cutting-edge ideas in the \textit{pretrain-finetune} process.
You'll train a Transformer to perform a task that involves accessing knowledge about the world -- knowledge which isn't provided via the task's training data (at least if you want to generalize outside the training set). You'll find that it more or less fails entirely at the task.
You'll then learn how to pretrain that Transformer on Wikipedia text that contains world knowledge, and find that finetuning that Transformer on the same knowledge-intensive task enables the model to access some of the knowledge learned at pretraining time.
You'll find that this enables models to perform considerably above chance on a held out development set.

%This part of the assignment is intended to be a soft introduction to research and practice in natural language processing, as well as .
%You'll start with a codebase that is fully functional for one purpose, and repurpose it for another task.

The code you're provided with is a fork of Andrej Karpathy's \href{https://github.com/karpathy/minGPT}{\mingpt}.
It's nicer than most research code in that it's relatively simple and transparent.
The ``GPT'' in \mingpt refers to the Transformer language model of OpenAI, originally described in \href{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}{this paper} \footnote{\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}}.

%Your overall goal is to repurpose this codebase with its Jupyter notebook for language modeling to build a small \textit{Text-to-text-transfer-transformer} (or, \textsc{T5})-style encoder-decoder model, 

As in previous assignments, you will want to develop on your machine locally, then run training on Azure. You can use the same conda environment from previous assignments for local development, and the same process for training on Azure (see the \textit{Practical Guide for Using the VM} section of the \href{https://docs.google.com/document/d/10J520Vnb1LnAMo0qgSYpG5cEEbomqQ371NIqg1IAv-4/edit?usp=sharing}{XCS224N Azure Guide} for a refresher). You might find the troubleshooting section useful if you see any issue in conda environment and GPU usage. Specifically, you'll still be running ``\texttt{conda activate XCS224N\_CUDA}'' on the Azure machine. You'll need around 2 hours for training, so budget your time accordingly!

Your work with this codebase is as follows:

\begin{enumerate}[(a)]
% \begin{parts}
% ISS
% \part[0]
\item \points{1a} \textbf{Review the minGPT demo code (no need to submit code or written)}\\

\textit{Note that you do not have to write any code or submit written answers for this part.}

In the \texttt{src/submission/mingpt-demo/} folder, there is a Jupyter notebook (\texttt{play\_char.ipynb}) that trains and samples from a Transformer language model.
Take a look at it locally on your computer and you might need to install Jupyter notebootk \texttt{pip install jupyter} or use vscode \footnote{\url{https://code.visualstudio.com/docs/datascience/jupyter-notebooks}} to get somewhat familiar with the code how it defines and trains models. \textit{You don't need to run the train locally, because training will take long time on CPU on your local environment.}
Some of the code you are writing below will be inspired by what you see in this notebook.

%If you want to run it, you'll need

%Demo code is ready to run; go to the \texttt{mingpt-demo/} folder, run the Jupyter notbook \texttt{play\_char.ipynb} and walk through the steps to train a Transformer language model on some text of your choice.\footnote{Feel free to change the transformer size, as well as the number of training epochs, so that you can train on your computer. This is just to get you somewhat familiar with this code!}
%Paste a short snippet of the generated text here, and concisely describe the domain (source of text) the language model was trained on.
%Look at the similarity between the snippet you generated and the text you trained the model on.
%Based on this similarity, make a brief (two-sentence) argument as to whether the model is likely to be \textit{overfit} or \textit{underfit} (or just right, but that's unlikely.)


% ISS
% \part[0]
\item \points{1b} \textbf{Read through \texttt{NameDataset} in \texttt{src/submission/dataset.py}, our dataset for reading name-birth place pairs.}\\
The task we'll be working on with our pretrained models is attempting to access the birth place of a notable person, as written in their Wikipedia page.
We'll think of this as a particularly simple form of question answering:
\begin{quote}
    \textit{Q: Where was \textit{[person]} born?}\\
    \textit{A: [place]}
\end{quote}
%In \texttt{mingpt/play\_char.ipynb}, you're provided with the class \texttt{CharDataset}, which provides input and target tensors for training the GPT model.
From now on, you'll be working with the \texttt{src/submission} folder. \textbf{The code in \texttt{mingpt-demo/} won't be changed or evaluated for this assignment.}
In \texttt{dataset.py}, 
%you'll take inspiration from \texttt{CharDataset}, and create the class \texttt{NameDataset}, which reads a TSV of name/place pairs and produces examples that we can feed to our Transformer model.
you'll find the the class \texttt{NameDataset}, which reads a TSV (tab-separated values) file of name/place pairs and produces examples of the above form that we can feed to our Transformer model.

To get a sense of the examples we'll be working with, if you run the following code, it'll load your \texttt{NameDataset} on the training set \texttt{birth\_places\_train.tsv} and print out a few examples.
\begin{lstlisting}[language=bash]
    cd src/submission
    python dataset.py namedata 
\end{lstlisting}

Note that you do not have to write any code or submit written answers for this part.

% \part[0]
\item \points{1c} \textbf{Define a \textit{span corruption} function for pretraining.}\\
In the file \texttt{src/submission/dataset.py}, implement the \texttt{\_\_getitem\_\_()} function for the dataset class \\
\texttt{CharCorruptionDataset}.
Follow the instructions provided in the comments in \texttt{dataset.py}.
Span corruption is explored in the \href{https://arxiv.org/pdf/1910.10683.pdf}{T5 paper} \footnote{\url{https://arxiv.org/pdf/1910.10683.pdf}}.
It randomly selects spans of text in a document and replaces them with unique tokens (noising).
Models take this noised text, and are required to output a pattern of each unique sentinel followed by the tokens that were replaced by that sentinel in the input.
In this question, you'll implement a simplification that only masks out a single sequence of characters.

This question will be graded via autograder based on your whether span corruption function implements some basic properties of our spec.
We'll instantiate the \texttt{CharCorruptionDataset} with our own data, and draw examples from it.
%Remember that you can play around with your new dataset class, perhaps draw some samples from it given some text.

To help you debug, if you run the following code, it'll sample a few examples from your \texttt{CharCorruptionDataset} on the pretraining dataset \texttt{wiki.txt} and print them out for you.
\begin{lstlisting}[language=bash]
    cd src/submission
    python dataset.py charcorruption
\end{lstlisting}

No written answer is required for this part.

%\part[5]
\item \points{1d} \textbf{Implement finetuning (without pretraining).}\\
Take a look at \texttt{src/submission/helper.py}, which is used by \texttt{src/run.py}.

It has some skeleton code you will implement to \textit{pretrain}, \textit{finetune} or \textit{evaluate} a model. For now, we'll focus on the finetuning function, in the case without pretraining.

Taking inspiration from the training code in the \texttt{src/submission/mingpt-demo/play\_char.ipynb} jupyter notebook file, write code to finetune a Transformer model on the name/birth place dataset, via examples from the \texttt{NameDataset} class. For now, implement the case without pretraining (i.e. create a model from scratch and train it on the birth-place prediction task from part (b)). You'll have to modify three sections, marked \texttt{[part d]} in the code: one to initialize the model, one to finetune it, and one to train it. Note that you only need to initialize the model in the case labeled ``vanilla'' for now (later in section (g), we will explore a model variant).
Use the hyperparameters for the \texttt{Trainer} specified in the \texttt{src/submission/helper.py} code.

Also take a look at the \textit{evaluation} code which has been implemented for you. It samples predictions from the trained model and calls \texttt{evaluate\_places()} to get the total percentage of correct place predictions. You will run this code in part (d) to evaluate your trained models.
%Also fill in the portion for \textit{evaluation}, using the function \texttt{evaluate\_places}.
%Here, take note of the code in \texttt{play\_char.ipynb} that samples from the trained model, and repurpose it to make a prediction with the model for the name/place task.

\textit{Note that this is an intermediate step for later portions, including Part~\ref{part_predictions_nopretraining}, which contains commands you can run to check your implementation. No written answer is required for this part.}
%; see there as well as \texttt{run.py}.


% ISS
% \part[10]
\item \points{1e}  \textbf{Make predictions (without pretraining).}\label{part_predictions_nopretraining}\\ 
Train your model on \texttt{birth\_places\_train.tsv}, and evaluate on \texttt{birth\_dev.tsv} and \texttt{birth\_test.tsv}. Specifically, you should now be able to run the following three commands:
\begin{lstlisting}[language=bash]
cd src

# Train on the names dataset
./run.sh vanilla_finetune_without_pretrain
        
# Evaluate on the dev set, writing out predictions
./run.sh vanilla_eval_dev_without_pretrain
        
# Evaluate on the test set, writing out predictions
./run.sh vanilla_eval_test_without_pretrain
\end{lstlisting}

Training will take less than 10 minutes (on Azure). Your grades will be based on the output files from the run.

Don't be surprised if the evaluation result is well below 10\%; we will be digging into why in Part 2. As a reference point, we want to also calculate the accuracy the model would have achieved if it had just predicted ``London'' as the birth place for everyone in the dev set. 

%For this part our submission script will be collecting \texttt{vanilla.model.params}, \texttt{vanilla.nopretrain.dev.predictions}, and \texttt{vanilla.nopretrain.test.predictions}.
%We've provided, in \texttt{utils.py}, the function \texttt{evaluate\_places}, which we'll use to compare your predicted birth places to the true ones.

%\part[10]
\item \points{1f} \textbf{Pretrain, finetune, and make predictions. Budget 1 hour for training.}\\
Now fill in the \textit{pretrain} portion of \texttt{src/submission/helper.py}, which will pretrain a model on the span corruption task. Additionally, modify your \textit{finetune} portion to handle finetuning in the case \textit{with} pretraining. In particular, if a path to a pretrained model is provided in the bash command, load this model before finetuning it on the birth-place prediction task.
% to read from the parameters trained during pretraining, if a path to those parameters
Pretrain your model on \texttt{wiki.txt} (which should take approximately one hour), finetune it on \texttt{NameDataset} and evaluate it. Specifically, you should be able to run the following four commands:

\begin{lstlisting}[language=bash]
cd src

# Pretrain the model
./run.sh vanilla_pretrain
        
# Finetune the model
./run.sh vanilla_finetune_with_pretrain
        
# Evaluate on the dev set; write to disk
./run.sh vanilla_eval_dev_with_pretrain
        
# Evaluate on the test set; write to disk
./run.sh vanilla_eval_test_with_pretrain
\end{lstlisting}

We expect the dev accuracy will be at least 10\%, and will expect a similar accuracy on the held out test set.

\pagebreak % :0 bad LaTeX form but I'm doing it --John

% \part[10]
\item \points{1g} \textbf{Different kind of position embeddings}\\

In the previous part, you used the vanilla Transformer model, which used learned positional embeddings. In Section \ref{sec:pos_enc}, you also learned about the sinusoidal positional embeddings used in the original Transformer paper. In this part, you'll implement a different kind of positional embedding, called \href{https://arxiv.org/abs/2104.09864}{\textit{RoPE} (Rotary Positional Embedding)}\footnote{\url{https://arxiv.org/abs/2104.09864}}.

RoPE is a fixed positional embedding that is designed to encode relative position rather than absolute position. The issue with absolute positions is that if the transformer won't perform well on context lengths (e.g. 1000) much larger than it was trained on (e.g. 128), because the distribution of the position embeddings will be very different from the ones it was trained on. Relative position embeddings like RoPE alleviate this issue.

Given a feature vector with two features $x^{(1)}_t$ and $x^{(2)}_t$ at position $t$ in the sequence, the RoPE positional embedding is defined as:
\begin{align*}
    \text{RoPE}(x^{(1)}_t, x^{(2)}_t, t) = \begin{pmatrix} \cos t\theta & -\sin t\theta \\ \sin t\theta  & \cos t\theta \end{pmatrix} \begin{pmatrix} x^{(1)}_t \\ x^{(2)}_t \end{pmatrix}
\end{align*}
where $\theta$ is a fixed angle. For two features, the RoPE operation corresponds to a 2D rotation of the features by an angle $t\theta$. Note that the angle is a function of the position $t$.

For a $d$ dimensional feature, RoPE is applied to each pair of features with an angle $\theta_i$ defined as $\theta_i = 10000^{-2(i-1)/d},\ i \in \{1, 2, \ldots, d/2\}$.
\begin{align}
    \label{eq:rope_matrix}
    \begin{pmatrix}
    \cos t\theta_1 & -\sin t\theta_1 & 0 & 0 & \cdots & 0 & 0\\
    \sin t\theta_1 & \cos t\theta_1 & 0 & 0 & \cdots & 0 & 0\\
    0 & 0 & \cos t\theta_2 & -\sin t\theta_2 & \cdots & 0 & 0\\
    0 & 0 & \sin t\theta_2 & \cos t\theta_2 & \cdots & 0 & 0\\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
    0 & 0 & 0 & 0 & \cdots & \cos t\theta_{d/2} & -\sin t\theta_{d/2}\\
    0 & 0 & 0 & 0 & \cdots & \sin t\theta_{d/2} & \cos t\theta_{d/2}
    \end{pmatrix}
    \begin{pmatrix}
        x^{(1)}_t \\ x^{(2)}_t \\ x^{(3)}_t \\ x^{(4)}_t \\ \vdots \\ x^{(d-1)}_t \\ x^{(d)}_t 
    \end{pmatrix}
\end{align}


Finally, instead of adding the positional embeddings to the input embeddings, RoPE is applied to the key and query vectors for each head in the attention block for all the Transformer layers.

% \begin{subparts}

% \subpart[2] 
\begin{enumerate}[label=\roman*.]

\item (2 points) Using the rotation interpretation, RoPE operation can be viewed as rotation of the complex number $x^{(1)}_t + i x^{(2)}_t$ by an angle $t\theta$. Recall that this corresponds to multiplication by $e^{it\theta} = \cos t\theta + i \sin t\theta$.

For higher dimensional feature vectors, this interpretation allows us to compute Equation~\ref{eq:rope_matrix} more efficiently. Specifically, we can rewrite the RoPE operation as an element-wise multiplication (denoted by $\odot$) of two vectors as follows:

\begin{align}\label{eq:rope_elementwise}
    \begin{pmatrix}
        \cos t\theta_1 + i\sin t\theta_1 \\
        \cos t\theta_2 + i\sin t\theta_2 \\
        \vdots \\
        \cos t\theta_{d/2} + i\sin t\theta_{d/2}
    \end{pmatrix}
    \odot
    \begin{pmatrix}
        x^{(1)}_t + i x^{(2)}_t \\
        x^{(3)}_t + i x^{(4)}_t \\
        \vdots \\
        x^{(d-1)}_t + i x^{(d)}_t
    \end{pmatrix}
\end{align}

Show that the elements of the vector in Equation~\ref{eq:rope_matrix} can be obtained from Equation~\ref{eq:rope_elementwise}. Note that some additional operations like reshaping are necessary to make the two expressions equal but you do not need to provide a detailed derivation for full points. 


% \subpart[1]
\item (2 point) \textbf{Relative Embeddings.} Now we will show that the dot product of the RoPE embeddings of two vectors at positions $t_1$ and $t_2$ depends on the relative position $t_1 - t_2$ only. 

For simiplicity, we will assume two dimensional feature vectors (eg. $[a, b]$) and work with their complex number representations (eg. $a + ib$).

Show that $\langle \text{RoPE}(z_1, t_1), \text{RoPE}(z_2, t_2) \rangle = \langle \text{RoPE}(z_1, t_1 - t_2), \text{RoPE}(z_2, 0) \rangle$ where $\langle \cdot, \cdot \rangle$ denotes the dot product and $\text{RoPE}(z, t)$ is the RoPE embedding of vector $z$ at position $t$.

(Hint: Dot product of vectors represented as complex numbers is given by $\langle z_1, z_2 \rangle = \Re(\overline{z_1} z_2)$. For a complex number $z = a + ib\ (a,b\in\R$), $\Re(z) = a$ indicates the real component of $z$ and $\bar{z} = a - ib$ is the complex conjugate of $z$.)

\end{enumerate}

\item \points{1h} \textbf{Write and try out a different kind of position embeddings (Budget about 1 hour for training)}\\

In the provided code, RoPE is implemented using the functions \texttt{precompute\_rotary\_emb} and \texttt{apply\_rotary\_emb} in \texttt{src/submission/attention.py}. You need to implement these functions and the parts of code marked \texttt{[part h]} in \texttt{src/submission/attention.py} and \texttt{src/run.py} to use RoPE in the model.

Train a model with RoPE on the span corruption task and finetune it on the birthplace prediction task. Specifically, you should be able to run the following four commands:

\begin{lstlisting}[language=bash]
    cd src

    # Pretrain the model
    ./run.sh rope_pretrain
            
    # Finetune the model
    ./run.sh rope_finetune_with_pretrain
            
    # Evaluate on the dev set; write to disk
    ./run.sh rope_eval_dev_with_pretrain
            
    # Evaluate on the test set; write to disk
    ./run.sh rope_eval_test_with_pretrain
    \end{lstlisting}

We'll score your model as to whether it gets at least 20\% accuracy on the dev set.

\end{enumerate}

% \begin{answer}
% % ### START CODE HERE ###
% % ### END CODE HERE ###
% \end{answer}
% \end{subparts}
% \end{parts}


\pagebreak

\textbf{Deliverables}

For this assignment, please submit the following files within the src/submission directory.
Update files \textbf{without directory structure}.

This includes:

\begin{enumerate}
        \item \texttt{src/submission/\_\_init\_\_.py}
        \item \texttt{src/submission/attention.py}
        \item \texttt{src/submission/dataset.py}
        \item \texttt{src/submission/helper.py}
        \item \texttt{src/submission/model.py}
        \item \texttt{src/submission/trainer.py}
        \item \texttt{src/submission/utils.py}
        \item \texttt{src/submission/vanilla.model.params}
        \item \texttt{src/submission/vanilla.nopretrain.dev.predictions}
        \item \texttt{src/submission/vanilla.nopretrain.test.predictions}
        \item \texttt{src/submission/vanilla.pretrain.params}
        \item \texttt{src/submission/vanilla.finetune.params}
        \item \texttt{src/submission/vanilla.pretrain.dev.predictions}
        \item \texttt{src/submission/vanilla.pretrain.test.predictions}
        \item \texttt{src/submission/rope.pretrain.params}
        \item \texttt{src/submission/rope.finetune.params}
        \item \texttt{src/submission/rope.pretrain.dev.predictions}
        \item \texttt{src/submission/rope.pretrain.test.predictions}
\end{enumerate}

We provide a script \texttt{src/collect\_submission.sh} to collect these files and create a zip file for submission. You can run the script on Linux/Mac/Windows(using Git Bash) systems, then submit the zip file to the assignment.

\begin{lstlisting}[language=bash]
    cd src
    bash ./collect_submission.sh
\end{lstlisting}