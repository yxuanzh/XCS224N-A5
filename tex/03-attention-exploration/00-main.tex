% \section{Attention exploration}
% \graphicspath{ {images/} }

\section{Attention exploration}
% \titledquestion{Attention exploration}[21]
\label{sec:analysis}
Multi-headed self-attention is the core modeling component of Transformers.
In this question, we'll get some practice working with the self-attention equations, and motivate why multi-headed self-attention can be preferable to single-headed self-attention.

Recall that attention can be viewed as an operation on a query $q\in\mathbb{R}^d$, a set of value vectors $\{v_1,\dots,v_n\}, v_i\in\mathbb{R}^d$, and a set of key vectors $\{k_1,\dots,k_n\}, k_i \in \mathbb{R}^d$, specified as follows:
\begin{align}
&c = \sum_{i=1}^{n} v_i \alpha_i \\
&\alpha_i = \frac{\exp(k_i^\top q)}{\sum_{j=1}^{n} \exp(k_j^\top q)}.
\end{align}
% where $\alpha_i$ are frequently called the ``attention weights'', and the output $c\in\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.
with $\alpha = \{\alpha_1, \ldots, \alpha_n\}$ termed the ``attention weights''. 
Observe that the output $c\in\mathbb{R}^d$ is an average over the value vectors weighted with respect to $\alpha$.

% \begin{parts}

%\textbf{Copying in attention:}

\begin{enumerate}[(a)]

% ISS
% \part[2] \textbf{Copying in attention:}

%\subpart[100]
\item \points{3a} \label{copying} \textbf{Copying in attention:} One advantage of attention is that it's particularly easy to ``copy'' a value vector to the output $c$. In this problem, we'll motivate why this is the case.


\begin{enumerate}[label=\roman*.]
    % \begin{subparts}
    \item (1 point) The distribution $\alpha$ is typically relatively ``diffuse''; the probability mass is spread out between many different $\alpha_i$. However, this is not always the case. \textbf{Describe} (in one sentence) under what conditions the categorical distribution $\alpha$ puts almost all of its weight on some $\alpha_j$, where $j \in \{1, \ldots, n\}$ (i.e. $\alpha_j \gg \sum_{i \neq j} \alpha_i$). What must be true about the query $q$ and/or the keys $\{k_1,\dots,k_n\}$?
    
    \item (1 point) Under the conditions you gave in (i), \textbf{describe} the output $c$. 
\end{enumerate}

% ISS
%\part[4] \textbf{An average of two:} \label{q_avg_of_two}
\item \points{3b} \textbf{An average of two:}

Instead of focusing on just one vector $v_j$, a Transformer model might want to incorporate information from \textit{multiple} source vectors.

Consider the case where we instead want to incorporate information from \textbf{two} vectors $v_a$ and $v_b$, with corresponding key vectors $k_a$ and $k_b$.
Assume that (1) all key vectors are orthogonal, so $k_i^\top k_j = 0$ for all $i \neq j$; and (2) all key vectors have norm $1$.
\textbf{Find an expression} for a query vector $q$ such that $c \approx \frac{1}{2}(v_a + v_b)$, and \textbf{justify your answer}.\footnote{Hint: while the softmax function will never \textit{exactly} average the two vectors, you can get close by using a large scalar multiple in the expression.} (Recall what you learned in part~\ref{copying}.)


% Consider a set of key vectors $\{k_1,\dots,k_n\}$ where all key vectors are perpendicular, that is $k_i \perp k_j$ for all $i\not= j$.
% Let $\|k_i\|=1$ for all $i$.
% Let $\{v_1,\dots,v_n\}$ be a set of arbitrary value vectors.
% Let $v_a,v_b\in\{v_1,\dots,v_n\}$ be two of the value vectors.
% Give an expression for a query vector $q$ such that the output $c$ is approximately equal to the average of $v_a$ and $v_b$, that is, $\frac{1}{2}(v_a+v_b)$.\footnote{Hint: while the softmax function will never \textit{exactly} average the two vectors, you can get close by using a large scalar multiple in the expression.} Note that you can reference the corresponding key vector of $v_a$ and $v_b$ as $k_a$ and $k_b$.

% \begin{answer}
% % ### START CODE HERE ###
% % ### END CODE HERE ###
% \end{answer}


% ISS
% \part[5]\textbf{Drawbacks of single-headed attention:} \label{q_problem_with_single_head}
\item \points{3c}  \textbf{Drawbacks of single-headed attention:} \label{q_problem_with_single_head}
In the previous part, we saw how it was \textit{possible} for a single-headed attention to focus equally on two values.
The same concept could easily be extended to any subset of values.
In this question we'll see why it's not a \textit{practical} solution.
Consider a set of key vectors $\{k_1,\dots,k_n\}$ that are now randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i$ are known to you, but the covariances $\Sigma_i$ are unknown (unless specified otherwise in the question).
Further, assume that the means $\mu_i$ are all perpendicular; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.

    % ISS
    \begin{enumerate}[label=\roman*.]
    % \begin{subparts}
    \item (1 point) Assume that the covariance matrices are $\Sigma_i = \alpha I, \forall i \in \{1, 2, \ldots, n\}$, for vanishingly small $\alpha$.
    Design a query $q$ in terms of the $\mu_i$ such that as before, $c\approx \frac{1}{2}(v_a + v_b)$, and provide a brief argument as to why it works.


    \item (2 point) Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue. Specifically, in some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\mu_a$ \footnote{Unlike the original Transformer, some newer Transformer models apply layer normalization before attention. In these pre-layernorm models, norms of keys cannot be too different which makes the situation in this question less likely to occur.}.
    
    As an example, let us consider a covariance for item $a$ as $\Sigma_a = \alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$ (as shown in figure \ref{ka_plausible}).
    Further, let $\Sigma_i = \alpha I$ for all $i \neq a$. %\footnote{Note that $\pi_i\pi_i^\top$ is an \textit{outer product}; a matrix in $\mathbb{R}^{d\times d}$. You can look up the definition, but to reason about what it means, consider how it behaves when you multiply vectors with it. In particular, $(\pi_a\pi_a^\top)\pi_a$ is equal to what? How about $(\pi_a\pi_a)^\top \pi_j$, for $a\not=j$?} 

    % ISS FIGURE INCLUDE ISSUE

    \begin{figure}[h]
        \centering
        \captionsetup{justification=centering,margin=2cm}
        \includegraphics[width=0.35\linewidth]{images/ka_plausible.png}
        \caption{The vector $\mu_a$ (shown here in 2D as an example), with the range of possible values of $k_a$ shown in red. As mentioned previously, $k_a$ points in roughly the same direction as $\mu_a$, but may have larger or smaller magnitude.}
        \label{ka_plausible}
    \end{figure}

    When you sample $\{k_1,\dots,k_n\}$ multiple times, and use the $q$ vector that you defined in part i., what do you expect the vector $c$ will look like qualitatively for different samples? Think about how it differs from part (i) and how $c$'s variance would be affected.

% \end{subparts}
    \end{enumerate}


% ISS
%\part[3] \textbf{Benefits of multi-headed attention:}
\item \points{3d} \label{q_multi_head} \textbf{Benefits of multi-headed attention:}
Now we'll see some of the power of multi-headed attention.
We'll consider a simple version of multi-headed attention which is identical to single-headed self-attention as we've presented it in this homework, except two query vectors ($q_1$ and $q_2$) are defined, which leads to a pair of vectors ($c_1$ and $c_2$), each the output of single-headed attention given its respective query vector.
The final output of the multi-headed attention is their average, $\frac{1}{2}(c_1+c_2)$.
As in question 3(\ref{q_problem_with_single_head}), consider a set of key vectors $\{k_1,\dots,k_n\}$ that are randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i$ are known to you, but the covariances $\Sigma_i$ are unknown.
Also as before, assume that the means $\mu_i$ are mutually orthogonal; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.
    % \begin{subparts}
    \begin{enumerate}[label=\roman*.]
    % \subpart[1]
    \item (1 point) Assume that the covariance matrices are $\Sigma_i=\alpha I$, for vanishingly small $\alpha$.
    Design $q_1$ and $q_2$ in terms of $\mu_i$ such that $c$ is approximately equal to $\frac{1}{2}(v_a+v_b)$. 
    Note that $q_1$ and $q_2$ should have different expressions. 

    % \subpart[2]
    \item (1 points) Assume that the covariance matrices are $\Sigma_a=\alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$, and $\Sigma_i=\alpha I$  for all $i \neq a$.
    Take the query vectors $q_1$ and $q_2$ that you designed in part i.
    What, qualitatively, do you expect the output $c$ to look like across different samples of the key vectors? Please briefly explain why. You can ignore cases in which $q_i^\top k_a < 0$.

    \end{enumerate}
    % \end{subparts}

\item \points{3e}  Based on part~\ref{q_multi_head}, briefly summarize how multi-headed attention overcomes the drawbacks of single-headed attention that you identified in part~\ref{q_problem_with_single_head}.


\end{enumerate}